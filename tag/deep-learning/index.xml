<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Deep Learning | Academic</title>
    <link>https://paritoshmittal12.github.io/tag/deep-learning/</link>
      <atom:link href="https://paritoshmittal12.github.io/tag/deep-learning/index.xml" rel="self" type="application/rss+xml" />
    <description>Deep Learning</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Mon, 06 Dec 2021 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://paritoshmittal12.github.io/media/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_3.png</url>
      <title>Deep Learning</title>
      <link>https://paritoshmittal12.github.io/tag/deep-learning/</link>
    </image>
    
    <item>
      <title>Non-sequential Autoregressive Shape Priors for 3D Completion, Reconstruction and Generation</title>
      <link>https://paritoshmittal12.github.io/project/3d-generation/</link>
      <pubDate>Mon, 06 Dec 2021 00:00:00 +0000</pubDate>
      <guid>https://paritoshmittal12.github.io/project/3d-generation/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Multi-Modal Multi-Hop Source Retrieval using Graph Convolutions</title>
      <link>https://paritoshmittal12.github.io/project/graphconv/</link>
      <pubDate>Wed, 01 Dec 2021 00:00:00 +0000</pubDate>
      <guid>https://paritoshmittal12.github.io/project/graphconv/</guid>
      <description>&lt;p&gt;Natural answers to general questions are often generated by aggregating information from multiple sources. However, most question answering approaches assume a single source of information (usually images or text). This is a weak assumption as information is often sparse and scattered. Next, the source of information can also be vision, text, speech, or any other modality. In this work, we move away from the simplistic assumptions of VQA and explore methods that can effectively capture the multimodal and multihop aspects of information retrieval. Specifically, we analyze the task of information source selection through the lens of Graph Convolution Neural Networks. We propose three independent methods which explore different graph arrangements and weighted connections across nodes. Our experiments and corresponding analysis highlight the prowess of graph-based methods in performing multihop reasoning even with primitive representations of input modalities. We also contrast our approach with existing baselines and transformer-based methods which by design fail to perform multihop reasoning for source selection task.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Inverting 3D Deep Learning Architectures</title>
      <link>https://paritoshmittal12.github.io/project/model-inversion/</link>
      <pubDate>Sat, 27 Nov 2021 00:00:00 +0000</pubDate>
      <guid>https://paritoshmittal12.github.io/project/model-inversion/</guid>
      <description>&lt;p&gt;Several applications of computer vision e.g. autonomous driving, warehouse management etc. are nearing deployment in real-world. However these approaches critically depend on &amp;lsquo;black-box&amp;rsquo; 3D neural networks. Interpreting and understanding these networks is an important and challenging problem for vision community. This project attempts to interpret the learning of 3D neural networks through the lens of model inversion. Specifically, we investigate what a 3D model learns by trying to re-create an optimal input based on a perceived &amp;lsquo;output&amp;rsquo;. Recent methods present solutions for inverting classification and detection networks, but only for 2D inputs. This project extend these approaches to 3D which is significantly more complex and ill-posed. We showcase results on inversion of 3D deep learning architectures for classification and detection and further analyse our findings.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
